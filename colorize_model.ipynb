{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FefpY6wSR1Am"
   },
   "source": [
    "##IMAGE COLORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSK07vLxKOEU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiSQ0sm5KU49",
    "outputId": "c4558473-b8bb-4792-bb90-92842215ed23",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTjkNxNtKd9i",
    "outputId": "8020b292-f90f-4544-a049-fb25e53d1c6c"
   },
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9XOE1J8KmFQ"
   },
   "outputs": [],
   "source": [
    "# Define the colorization model (U-Net-like architecture, no sigmoid at output)\n",
    "class ColorizationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.final = nn.Conv2d(64, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "\n",
    "        # Decoder\n",
    "        d1 = self.up1(e3)\n",
    "        # Crop e2 to match d1 if needed\n",
    "        if d1.shape[2:] != e2.shape[2:]:\n",
    "            e2 = e2[:, :, :d1.shape[2], :d1.shape[3]]\n",
    "        d1 = torch.cat([d1, e2], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        d2 = self.up2(d1)\n",
    "        # Crop e1 to match d2 if needed\n",
    "        if d2.shape[2:] != e1.shape[2:]:\n",
    "            e1 = e1[:, :, :d2.shape[2], :d2.shape[3]]\n",
    "        d2 = torch.cat([d2, e1], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        out = self.final(d2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkPwdCQwKsQk"
   },
   "outputs": [],
   "source": [
    "model = ColorizationNet().to(device)  # device is defined above\n",
    "\n",
    "# Optionally load model weights if available\n",
    "import os\n",
    "weights_path = \"colorization_model_weights.pth\"\n",
    "if os.path.exists(weights_path):\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    print(f\"Loaded model weights from {weights_path}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Changed from Adagrad to Adam\n",
    "\n",
    "# Convert RGB image to grayscale\n",
    "def rgb_to_gray(img):\n",
    "    return img.mean(dim=1, keepdim=True) # bw has 1 dimension, keep-dimensions\n",
    "\n",
    "    '''\n",
    "    - model.parameters() provides the optimizer with access to the model's adjustable parameters.\n",
    "    - lr=0.001 sets the learning rate, controlling the step size the optimizer takes when updating the model's parameters. A smaller learning rate leads to slower but potentially more stable training.\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "VGCn66P_KvaT",
    "outputId": "b7dcb0ef-8cbd-4e6f-e405-7dc552339279"
   },
   "outputs": [],
   "source": [
    "# Only train if not loading weights\n",
    "skip_training = os.path.exists(weights_path)\n",
    "if not skip_training:\n",
    "    # Training loop\n",
    "    EPOCHS = 50\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, (images, _) in enumerate(train_loader):\n",
    "            grayscale_images = rgb_to_gray(images).to(device)\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(grayscale_images)\n",
    "            outputs = torch.clamp(outputs, 0.0, 1.0)\n",
    "            loss = criterion(outputs, images)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics and gradient info\n",
    "            if i % 100 == 0:\n",
    "                grad_means = []\n",
    "                grad_stds = []\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        grad_means.append(param.grad.mean().item())\n",
    "                        grad_stds.append(param.grad.std().item())\n",
    "                grad_mean = sum(grad_means)/len(grad_means) if grad_means else 0.0\n",
    "                grad_std = sum(grad_stds)/len(grad_stds) if grad_stds else 0.0\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Grad mean: {grad_mean:.6f}, Grad std: {grad_std:.6f}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    # Save model weights after training\n",
    "    torch.save(model.state_dict(), \"colorization_model_weights.pth\")\n",
    "    print(\"Model weights saved to colorization_model_weights.pth\")\n",
    "else:\n",
    "    print(\"Weights loaded, skipping training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colorize your own black and white image ---\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to your grayscale image (change this to your file)\n",
    "img_path = r\"C:\\Users\\thedy\\Downloads\\girl-nikon-z-series-tips-nikon-cameras-lenses-accessories.webp\"\n",
    "\n",
    "# Load image as grayscale\n",
    "img = Image.open(img_path).convert('L')  # 'L' mode = grayscale\n",
    "\n",
    "# Convert to tensor and normalize to [0,1]\n",
    "to_tensor = transforms.ToTensor()  # outputs shape (1, H, W)\n",
    "gray_tensor = to_tensor(img).unsqueeze(0)  # shape (1, 1, H, W)\n",
    "\n",
    "# Move to device\n",
    "gray_tensor = gray_tensor.to(device)\n",
    "\n",
    "# Run through model\n",
    "with torch.no_grad():\n",
    "    colorized = model(gray_tensor)\n",
    "    colorized = colorized.squeeze(0).cpu()  # shape (3, H, W)\n",
    "\n",
    "# Convert to numpy for display\n",
    "colorized_np = colorized.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Show original and colorized\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axs[0].imshow(img, cmap='gray')\n",
    "axs[0].set_title('Input Grayscale')\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(colorized_np)\n",
    "axs[1].set_title('Colorized Output')\n",
    "axs[1].axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
